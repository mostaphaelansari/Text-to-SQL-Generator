{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3160014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619e05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import pipeline\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d52588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "data_path = os.path.join(project_root, 'text_to_sql_generator', 'data', 'processed', 'text2sql_clean.csv')\n",
    "cleaned_dataset = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50efa960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the total volume of timber sold by eac...</td>\n",
       "      <td>SELECT salesperson_id, name, SUM(volume) as to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List all the unique equipment types and their ...</td>\n",
       "      <td>SELECT equipment_type, SUM(maintenance_frequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many marine species are found in the South...</td>\n",
       "      <td>SELECT COUNT(*) FROM marine_species WHERE loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the total trade value and average pric...</td>\n",
       "      <td>SELECT trader_id, stock, SUM(price * quantity)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find the energy efficiency upgrades with the h...</td>\n",
       "      <td>SELECT type, cost FROM (SELECT type, cost, ROW...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sql_prompt  \\\n",
       "0  What is the total volume of timber sold by eac...   \n",
       "1  List all the unique equipment types and their ...   \n",
       "2  How many marine species are found in the South...   \n",
       "3  What is the total trade value and average pric...   \n",
       "4  Find the energy efficiency upgrades with the h...   \n",
       "\n",
       "                                                 sql  \n",
       "0  SELECT salesperson_id, name, SUM(volume) as to...  \n",
       "1  SELECT equipment_type, SUM(maintenance_frequen...  \n",
       "2  SELECT COUNT(*) FROM marine_species WHERE loca...  \n",
       "3  SELECT trader_id, stock, SUM(price * quantity)...  \n",
       "4  SELECT type, cost FROM (SELECT type, cost, ROW...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1663380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(cleaned_dataset)\n",
    "\n",
    "# Format input/output pairs\n",
    "PREFIX = \"translate to SQL: \"\n",
    "df[\"input_text\"] = PREFIX + df[\"sql_prompt\"]\n",
    "df[\"target_text\"] = df[\"sql\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd60ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a270d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"target_text\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca7ecb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c7c409ddb44ce59dccc2bee6b1be0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c2a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-sql-finetuned\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10,  # or less\n",
    "    max_steps=5000,       # override if needed\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa71b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21210982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosta\\AppData\\Local\\Temp\\ipykernel_113948\\3049639269.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,              # Your model (T5, BART, etc.)\n",
    "    args=training_args,       # The training arguments you defined\n",
    "    train_dataset=tokenized_dataset,  # Your preprocessed training data\n",
    "    tokenizer=tokenizer,      # Tokenizer for your model\n",
    "    data_collator=data_collator,  # Handles batching and padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosta\\AppData\\Local\\Temp\\ipykernel_99684\\739440759.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 07:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.348500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.4589886322021484, metrics={'train_runtime': 422.8574, 'train_samples_per_second': 23.649, 'train_steps_per_second': 11.824, 'total_flos': 1353418014720000.0, 'train_loss': 0.4589886322021484, 'epoch': 0.1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()  # ✅ This is the correct method to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085c8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 06:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.283700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.2872958251953125, metrics={'train_runtime': 410.7414, 'train_samples_per_second': 24.346, 'train_steps_per_second': 12.173, 'total_flos': 1353418014720000.0, 'train_loss': 0.2872958251953125, 'epoch': 0.1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851af125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT salesperson, SUM(volume) as total_volume FROM sales GROUP BY salesperson;\n"
     ]
    }
   ],
   "source": [
    "inference = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "output = inference(\n",
    "    \"translate to SQL: What is the total volume of timber sold by each salesperson?\",\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a7508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-sql-finetuned\\\\tokenizer_config.json',\n",
       " 't5-sql-finetuned\\\\special_tokens_map.json',\n",
       " 't5-sql-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"t5-sql-finetuned\")\n",
    "tokenizer.save_pretrained(\"t5-sql-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad51e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint-1000', 'checkpoint-1500', 'checkpoint-15500', 'checkpoint-16000', 'checkpoint-2000', 'checkpoint-2500', 'checkpoint-3000', 'checkpoint-3500', 'checkpoint-4000', 'checkpoint-4500', 'checkpoint-500', 'checkpoint-5000', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./t5-sql-finetuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a95fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the config file to see model details\n",
    "with open(\"./t5-sql-finetuned/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "# This might give clues about the original model\n",
    "print(config.get(\"_name_or_path\", \"\"))\n",
    "print(config.get(\"model_type\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead1d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m model = T5ForConditionalGeneration.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./t5-sql-finetuned\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the standard T5 tokenizer\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = T5Tokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./t5-sql-finetuned\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2062\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2059\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_pretrained(\n\u001b[32m   2063\u001b[39m     resolved_vocab_files,\n\u001b[32m   2064\u001b[39m     pretrained_model_name_or_path,\n\u001b[32m   2065\u001b[39m     init_configuration,\n\u001b[32m   2066\u001b[39m     *init_inputs,\n\u001b[32m   2067\u001b[39m     token=token,\n\u001b[32m   2068\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   2069\u001b[39m     local_files_only=local_files_only,\n\u001b[32m   2070\u001b[39m     _commit_hash=commit_hash,\n\u001b[32m   2071\u001b[39m     _is_local=is_local,\n\u001b[32m   2072\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m   2073\u001b[39m     **kwargs,\n\u001b[32m   2074\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2302\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2304\u001b[39m     logger.info(\n\u001b[32m   2305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2307\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:150\u001b[39m, in \u001b[36mT5Tokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28mself\u001b[39m._extra_ids = extra_ids\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.sp_model = spm.SentencePieceProcessor(**\u001b[38;5;28mself\u001b[39m.sp_model_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28mself\u001b[39m.sp_model.Load(vocab_file)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m additional_special_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    153\u001b[39m     extra_tokens = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m additional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m<extra_id_\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[39m, in \u001b[36mSentencePieceProcessor.Load\u001b[39m\u001b[34m(self, model_file, model_proto)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[32m    960\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.LoadFromSerializedProto(model_proto)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.LoadFromFile(model_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[39m, in \u001b[36mSentencePieceProcessor.LoadFromFile\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece.SentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[31mTypeError\u001b[39m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5-sql-finetuned\").to(\"cuda\")\n",
    "\n",
    "# Use the standard T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5-sql-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984f76a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3b0fa9bed54edf8e5c6742e039483b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/600 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mosta\\anaconda3\\envs\\SQLtoTEXT\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mosta\\.cache\\huggingface\\hub\\datasets--lamini--spider_text_to_sql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4225255c9dc44cafbbb931b1e6375103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-36a24700f19484dc.parquet:   0%|          | 0.00/932k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe73bd0f00a42bf9648fc1ad59c2783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-fa01d04c056ac579.parquet:   0%|          | 0.00/122k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2240c609027f4f4dbae6db2d072e8da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643890b10fb3457ba4b1b683b652dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lamini/spider_text_to_sql\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SQLtoTEXT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
